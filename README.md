# DubTok
DubTok, an application that automatically dubs your videos into any language. Simply upload your video, choose your desired language, and our solution delivers high-quality, seamlessly dubbed content!

# Demo

Try out Dubtok online: https://dubtok-forked.vercel.app/

## Folder Structure
Our project is broken up into the following folders
```
.
â”œâ”€â”€ ğŸ“‚ ai/              # Contains code related to ai
â”œâ”€â”€ ğŸ“‚ backend/         # Contains code related to the backend
â”œâ”€â”€ ğŸ“‚ frontend/        # Contains code related to the frontend
â”œâ”€â”€ ğŸ“‚ sample_videos/   # Sample videos for testing the application
â”œâ”€â”€ ğŸ“‚ uploads/         # Folder used for storing uploaded files
â”œâ”€â”€ âš™ .gitignore
â””â”€â”€ ğŸ“” README.md
```

We split the AI, backend, and the frontend each into their own directories. We also uploaded some sample videos which can be used for testing the application.

## Sequence diagram
Below is a sequence diagram detailing our application operations.

![Sequence diagram detailing our application operations](./images/seq_dig.png)


## Setting up of environment variables
Before running the application, you would need to setup the environment variables containing the API keys to the services used in our project. You would need an OpenAI API key and a PlayHT API key. To setup the environment variables, create a .env file in the root folder.
```
.
â”œâ”€â”€ ğŸ“‚ ai/              
â”œâ”€â”€ ğŸ“‚ backend/         
â”œâ”€â”€ ğŸ“‚ frontend/        
â”œâ”€â”€ ğŸ“‚ sample_videos/   
â”œâ”€â”€ ğŸ“‚ uploads/         
â”œâ”€â”€ âš™ .gitignore
â”œâ”€â”€ ğŸ“” README.md
â””â”€â”€ âš™ .env             # Create the .env file here!
``` 

Inside the .env file, include the following parameters.
```
OPENAI_API_KEY=<INSERT YOUR OPENAI API KEY HERE!>
PLAYHT_USERID=<INSERT YOUR PLAYHT USERID HERE!>
PLAYHT_APIKEY=<INSERT YOUR PLAYHT API KEY HERE!>
```

You would also need to configure AWS and setup a S3 bucket to use our application. Our application uses the bucket name "tiktoktechjam-2024". Create an S3 bucket with the same name, and make sure `Block public and cross-account access to buckets and objects through any public bucket or access point policies` is unchecked. Create a bucket policy allowing public read requests such as the following:
```
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Sid": "PublicReadGetObject",
            "Effect": "Allow",
            "Principal": "*",
            "Action": "s3:GetObject",
            "Resource": "arn:aws:s3:::tiktoktechjam-2024/*"
        }
    ]
}
```

You will then need to configure your AWS credentials on your local machine to access the bucket. You can follow [this](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html) tutorial to setup your AWS credentials. 

# Frontend
Our frontend contains the following files
```
.
â””â”€â”€ ğŸ“‚ frontend/
    â”œâ”€â”€ ğŸ“‚ public/
    â”œâ”€â”€ ğŸ“‚ src/app/
    â”‚   â”œâ”€â”€ ğŸ“‚ components/     # Contains the React components
    â”‚   â”œâ”€â”€ ğŸ“‚ config/  
    â”‚   â”œâ”€â”€ ğŸ“‚ demo/
    â”‚   â”œâ”€â”€ ğŸ“‚ test/
    â”‚   â”œâ”€â”€ ğŸ“‚ utils/
    â”‚   â”œâ”€â”€ ğŸ”½ favicon.ico
    â”‚   â”œâ”€â”€ ğŸ¨ globals.css
    â”‚   â”œâ”€â”€ ğŸ—’ layout.js
    â”‚   â”œâ”€â”€ ğŸ—’ page.js
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ ğŸ—’ tailwind.config.js   # Configuration file for Tailwind CSS
    â””â”€â”€ ...
```

To run the frontend, navigate into the frontend folder and install the required dependencies
```
npm i
```
You will require Node.js to install these dependencies.

After installing the required dependencies, run the frontend with
``` 
npm start
```

# Backend
Our backend contains the following files
```
.
â””â”€â”€ ğŸ“‚ backend/
    â”œâ”€â”€ ğŸ“‚ db/
    â”‚   â”œâ”€â”€ ğŸ __init__.py
    â”‚   â”œâ”€â”€ ğŸ crud.py      # Contains the CRUD operations for the database
    â”‚   â”œâ”€â”€ ğŸ database.py  # Connects to the SQLite database
    â”‚   â”œâ”€â”€ ğŸ models.py    # Contains the ORM code
    â”‚   â””â”€â”€ ğŸ schemas.py   # Schemas to validate the ORM data structures
    â”œâ”€â”€ ğŸ“‚ services/
    â”‚   â””â”€â”€ ğŸ video_audio_processor.py     # Splits the audio from the video
    â”œâ”€â”€ ğŸ“‚ tests/
    â”‚   â””â”€â”€ ğŸ api_test.py  # Contains test code for the API endpoints
    â”œâ”€â”€ ğŸ main.py          # Main code to run the backend server
    â””â”€â”€ ğŸ“ requirements.txt
```

To run the backend server, navigate into the backend folder and install the required dependencies in requirements.py. You can do so in by creating a virtual environment first and installing the requirements with 
```
pip install -r requirements.txt
```

After installing, simply run the server with the following command
```
fastapi dev main.py
```
This would run start the development server

# AI
Our AI folder contains the following files
```
.
â””â”€â”€ ğŸ“‚ ai/
    â”œâ”€â”€ ğŸ“‚ chunks/                  # Folder used for storing temporary files 
    â””â”€â”€ ğŸ“‚ output_audio/            # Folder used for storing temporary output files
        â”œâ”€â”€ ğŸ __init__.py
        â”œâ”€â”€ ğŸ ai_service.py        
        â”œâ”€â”€ ğŸ del_voice_clone.py
        â”œâ”€â”€ ğŸ fix_tts.py
        â”œâ”€â”€ ğŸ newtest.py
        â”œâ”€â”€ ğŸ openai_setup.py      # Loads the OpenAI API key
        â”œâ”€â”€ ğŸ preprocess_audio.py
        â”œâ”€â”€ ğŸ“ requirements.txt
        â”œâ”€â”€ ğŸ stitch.py            
        â”œâ”€â”€ ğŸ sync_translation.py
        â”œâ”€â”€ ğŸ transcribe.py        # Transcribe a given audio file
        â”œâ”€â”€ ğŸ translate.py         # Translates the transcript to the target language
        â”œâ”€â”€ ğŸ translated_chunks.py 
        â”œâ”€â”€ ğŸ tts.py
        â””â”€â”€ ğŸ voice_clone.py
```

The AI folder contains the pipeline for the audio dubbing process. Our audio dubbing pipeline is organized into several steps:

1. **Audio and Video Separation:** Using FFMpeg, the video is isolated from the audio file. 

2. **Audio Separation:** Utilizing Demucs, the service separates audio into vocals and non-vocals, crucial for creating a clear voice clone and assembling dubbed audio.

3. **Transcription and Timestamping:** OpenAI's API transcribes speech and provides word timestamps, essential for subsequent translation and voice cloning.

4. **Translation:** The transcribed text is translated into the target language using OpenAI's API, preparing it for voice cloning.

5. **Voice Cloning:** PlayHT's API clones the speaker's voice using the isolated vocal track, ensuring authenticity in the dubbed audio.

6. **Voice Dubbing:** Using translated transcripts and cloned voice, PlayHT generates sentence-level voice dubs, maintaining narrative coherence.

7. **Timing Adjustment:** Each dubbed sentence is meticulously adjusted to match the original's timing without altering pitch, ensuring synchronization with the video.

8. **Assembly:** Dubbed sentences are stitched together to form the complete dubbed audio, mixed with non-vocal audio to produce the final dubbed video soundtrack.

9. **Final Output:** The dubbed audio track is reassembled with the isolated video file, resulting in the completed dubbed video ready for distribution.
